{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "#31_Lab_Molecular_Property(logP)_Prediction_with_GCN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOMkA9027qvKhx816cXBW0O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/young-hwanlee/stand_alone_deep_learning/blob/main/%2331_Lab_Molecular_Property(logP)_Prediction_with_GCN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jnCFzTGE_8u"
      },
      "outputs": [],
      "source": [
        "!curl -LO  https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
        "\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.6/site-packages/')\n",
        "\n",
        "!conda install -y -c rdkit rdkit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -o ZINC.smiles https://raw.githubusercontent.com/heartcored98/Standalone-DeepLearning/master/Lec9/ZINC.smiles\n",
        "!curl -o vocab.npy https://raw.githubusercontent.com/heartcored98/Standalone-DeepLearning/master/Lec9/vocab.npy"
      ],
      "metadata": {
        "id": "uRm_yt1P7xH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import sys\n",
        "import time\n",
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from rdkit import Chem, DataStructs\n",
        "from rdkit.Chem import AllChem\n",
        "from rdkit.Chem.Crippen import MolLogP\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from tqdm import tnrange, tqdm_notebook\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "sgX95ykH7xMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paser = argparse.ArgumentParser()\n",
        "args = paser.parse_args(\"\")\n",
        "args.seed = 123\n",
        "args.val_size = 0.1\n",
        "args.test_size = 0.1\n",
        "args.shuffle = True"
      ],
      "metadata": {
        "id": "iWm4aSwC7xQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "else:\n",
        "    torch.set_default_tensor_type('torch.FloatTensor')"
      ],
      "metadata": {
        "id": "7lyLOEfs7xUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_ZINC_smiles(file_name, num_mol):\n",
        "    f = open(file_name, 'r')\n",
        "    contents = f.readlines()\n",
        "\n",
        "    smi_list = []\n",
        "    logP_list = []\n",
        "\n",
        "    for i in tqdm_notebook(range(num_mol), desc='Reading Data'):\n",
        "        smi = contents[i].strip()\n",
        "        m = Chem.MolFromSmiles(smi)\n",
        "        smi_list.append(smi)\n",
        "        logP_list.append(MolLogP(m))\n",
        "\n",
        "    logP_list = np.asarray(logP_list).astype(float)\n",
        "\n",
        "    return smi_list, logP_list\n",
        "\n",
        "\n",
        "def smiles_to_onehot(smi_list):\n",
        "    def smiles_to_vector(smiles, vocab, max_length):\n",
        "        while len(smiles) < max_length:\n",
        "            smiles += \" \"\n",
        "        vector = [vocab.index(str(x)) for x in smiles]\n",
        "        one_hot = np.zeros((len(vocab), max_length), dtype=int)\n",
        "        for i, elm in enumerate(vector):\n",
        "            one_hot[elm][i] = 1\n",
        "        return one_hot\n",
        "\n",
        "    vocab = np.load('./vocab.npy')\n",
        "    smi_total = []\n",
        "\n",
        "    for i, smi in tqdm_notebook(enumerate(smi_list), desc='Converting to One Hot'):\n",
        "        smi_onehot = smiles_to_vector(smi, list(vocab), 120)\n",
        "        smi_total.append(smi_onehot)\n",
        "\n",
        "    return np.asarray(smi_total)\n",
        "\n",
        "def convert_to_graph(smiles_list):\n",
        "    adj = []\n",
        "    adj_norm = []\n",
        "    features = []\n",
        "    maxNumAtoms = 50\n",
        "    for i in tqdm_notebook(smiles_list, desc='Converting to Graph'):\n",
        "        # Mol\n",
        "        iMol = Chem.MolFromSmiles(i.strip())\n",
        "        #Adj\n",
        "        iAdjTmp = Chem.rdmolops.GetAdjacencyMatrix(iMol)\n",
        "        # Feature\n",
        "        if( iAdjTmp.shape[0] <= maxNumAtoms):\n",
        "            # Feature-preprocessing\n",
        "            iFeature = np.zeros((maxNumAtoms, 58))\n",
        "            iFeatureTmp = []\n",
        "            for atom in iMol.GetAtoms():\n",
        "                iFeatureTmp.append( atom_feature(atom) ) ### atom features only\n",
        "            iFeature[0:len(iFeatureTmp), 0:58] = iFeatureTmp ### 0 padding for feature-set\n",
        "            features.append(iFeature)\n",
        "\n",
        "            # Adj-preprocessing\n",
        "            iAdj = np.zeros((maxNumAtoms, maxNumAtoms))\n",
        "            iAdj[0:len(iFeatureTmp), 0:len(iFeatureTmp)] = iAdjTmp + np.eye(len(iFeatureTmp))\n",
        "            adj.append(np.asarray(iAdj))\n",
        "    features = np.asarray(features)\n",
        "\n",
        "    return features, adj\n",
        "    \n",
        "def atom_feature(atom):\n",
        "    return np.array(one_of_k_encoding_unk(atom.GetSymbol(),\n",
        "                                      ['C', 'N', 'O', 'S', 'F', 'H', 'Si', 'P', 'Cl', 'Br',\n",
        "                                       'Li', 'Na', 'K', 'Mg', 'Ca', 'Fe', 'As', 'Al', 'I', 'B',\n",
        "                                       'V', 'Tl', 'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn',\n",
        "                                       'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'Mn', 'Cr', 'Pt', 'Hg', 'Pb']) +\n",
        "                    one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5]) +\n",
        "                    one_of_k_encoding_unk(atom.GetTotalNumHs(), [0, 1, 2, 3, 4]) +\n",
        "                    one_of_k_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5]) +\n",
        "                    [atom.GetIsAromatic()])    # (40, 6, 5, 6, 1)\n",
        "\n",
        "def one_of_k_encoding(x, allowable_set):\n",
        "    if x not in allowable_set:\n",
        "        raise Exception(\"input {0} not in allowable set{1}:\".format(x, allowable_set))\n",
        "    return list(map(lambda s: x == s, allowable_set))\n",
        "\n",
        "def one_of_k_encoding_unk(x, allowable_set):\n",
        "    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n",
        "    if x not in allowable_set:\n",
        "        x = allowable_set[-1]\n",
        "    return list(map(lambda s: x == s, allowable_set))"
      ],
      "metadata": {
        "id": "mXD5fhI90yPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_smi, list_logP = read_ZINC_smiles('ZINC.smiles', 10000)\n",
        "list_feature, list_adj = convert_to_graph(list_smi)"
      ],
      "metadata": {
        "id": "aiNPO8va0yM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GCNDataset(Dataset):\n",
        "    def __init__(self, list_feature, list_adj, list_logP):\n",
        "        self.list_feature = list_feature\n",
        "        self.list_adj = list_adj\n",
        "        self.list_logP = list_logP\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.list_feature)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.list_feature[index], self.list_adj[index], self.list_logP[index]\n",
        "\n",
        "\n",
        "def partition(list_feature, list_adj, list_logP, args):\n",
        "    num_total = list_feature.shape[0]\n",
        "    num_train = int(num_total * (1 - args.test_size - args.val_size))\n",
        "    num_val = int(num_total * args.val_size)\n",
        "    num_test = int(num_total * args.test_size)\n",
        "\n",
        "    feature_train = list_feature[:num_train]\n",
        "    adj_train = list_adj[:num_train]\n",
        "    logP_train = list_logP[:num_train]\n",
        "    feature_val = list_feature[num_train:num_train + num_val]\n",
        "    adj_val = list_adj[num_train:num_train + num_val]\n",
        "    logP_val = list_logP[num_train:num_train + num_val]\n",
        "    feature_test = list_feature[num_total - num_test:]\n",
        "    adj_test = list_adj[num_total - num_test:]\n",
        "    logP_test = list_logP[num_total - num_test:]\n",
        "        \n",
        "    train_set = GCNDataset(feature_train, adj_train, logP_train)\n",
        "    val_set = GCNDataset(feature_val, adj_val, logP_val)\n",
        "    test_set = GCNDataset(feature_test, adj_test, logP_test)\n",
        "\n",
        "    partition = {\n",
        "        'train': train_set,\n",
        "        'val': val_set,\n",
        "        'test': test_set\n",
        "    }\n",
        "\n",
        "    return partition"
      ],
      "metadata": {
        "id": "UAxFYXBc0yKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_partition = partition(list_feature, list_adj, list_logP, args)"
      ],
      "metadata": {
        "id": "DnoNPUqu0yHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GCNLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_dim, out_dim, n_atom, act=None, bn=False):\n",
        "        super(GCNLayer, self).__init__()\n",
        "        \n",
        "        self.use_bn = bn\n",
        "        self.linear = nn.Linear(in_dim, out_dim)\n",
        "        nn.init.xavier_uniform_(self.linear.weight)\n",
        "        self.bn = nn.BatchNorm1d(n_atom)\n",
        "        self.activation = act\n",
        "        \n",
        "    def forward(self, x, adj):\n",
        "        out = self.linear(x)\n",
        "        out = torch.matmul(adj, out)\n",
        "        if self.use_bn:\n",
        "            out = self.bn(out)\n",
        "        if self.activation != None:\n",
        "            out = self.activation(out)\n",
        "        return out, adj"
      ],
      "metadata": {
        "id": "P6W-Xe6r0yCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipConnection(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super(SkipConnection, self).__init__()\n",
        "        \n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        \n",
        "        self.linear = nn.Linear(in_dim, out_dim, bias=False)\n",
        "        \n",
        "    def forward(self, in_x, out_x):\n",
        "        if (self.in_dim != self.out_dim):\n",
        "            in_x = self.linear(in_x)\n",
        "        out = in_x + out_x\n",
        "        return out"
      ],
      "metadata": {
        "id": "GJALBMNR0x8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GatedSkipConnection(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super(GatedSkipConnection, self).__init__()\n",
        "        \n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        \n",
        "        self.linear = nn.Linear(in_dim, out_dim, bias=False)\n",
        "        self.linear_coef_in = nn.Linear(out_dim, out_dim)\n",
        "        self.linear_coef_out = nn.Linear(out_dim, out_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, in_x, out_x):\n",
        "        if (self.in_dim != self.out_dim):\n",
        "            in_x = self.linear(in_x)\n",
        "        z = self.gate_coefficient(in_x, out_x)\n",
        "        out = torch.mul(z, out_x) + torch.mul(1.0-z, in_x)\n",
        "        return out\n",
        "            \n",
        "    def gate_coefficient(self, in_x, out_x):\n",
        "        x1 = self.linear_coef_in(in_x)\n",
        "        x2 = self.linear_coef_out(out_x)\n",
        "        return self.sigmoid(x1+x2)"
      ],
      "metadata": {
        "id": "JvptMu4t1BTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GCNBlock(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_layer, in_dim, hidden_dim, out_dim, n_atom, bn=True, sc='gsc'):\n",
        "        super(GCNBlock, self).__init__()\n",
        "        \n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(n_layer):\n",
        "            self.layers.append(GCNLayer(in_dim if i==0 else hidden_dim,\n",
        "                                        out_dim if i==n_layer-1 else hidden_dim,\n",
        "                                        n_atom,\n",
        "                                        nn.ReLU() if i!=n_layer-1 else None,\n",
        "                                        bn))\n",
        "        self.relu = nn.ReLU()\n",
        "        if sc=='gsc':\n",
        "            self.sc = GatedSkipConnection(in_dim, out_dim)\n",
        "        elif sc=='sc':\n",
        "            self.sc = SkipConnection(in_dim, out_dim)\n",
        "        elif sc=='no':\n",
        "            self.sc = None\n",
        "        else:\n",
        "            assert False, \"Wrong sc type.\"\n",
        "        \n",
        "    def forward(self, x, adj):\n",
        "        residual = x\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            out, adj = layer((x if i==0 else out), adj)\n",
        "        if self.sc != None:\n",
        "            out = self.sc(residual, out)\n",
        "        out = self.relu(out)\n",
        "        return out, adj"
      ],
      "metadata": {
        "id": "elUapBi71BN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReadOut(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_dim, out_dim, act=None):\n",
        "        super(ReadOut, self).__init__()\n",
        "        \n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim= out_dim\n",
        "        \n",
        "        self.linear = nn.Linear(self.in_dim, \n",
        "                                self.out_dim)\n",
        "        nn.init.xavier_uniform_(self.linear.weight)\n",
        "        self.activation = act\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        out = torch.sum(out, 1)\n",
        "        if self.activation != None:\n",
        "            out = self.activation(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "u3ML4V-31K2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Predictor(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_dim, out_dim, act=None):\n",
        "        super(Predictor, self).__init__()\n",
        "        \n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        \n",
        "        self.linear = nn.Linear(self.in_dim,\n",
        "                                self.out_dim)\n",
        "        nn.init.xavier_uniform_(self.linear.weight)\n",
        "        self.activation = act\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        if self.activation != None:\n",
        "            out = self.activation(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "4BcnOEbj1Ku4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GCNNet(nn.Module):\n",
        "    \n",
        "    def __init__(self, args):\n",
        "        super(GCNNet, self).__init__()\n",
        "        \n",
        "        self.blocks = nn.ModuleList()\n",
        "        for i in range(args.n_block):\n",
        "            self.blocks.append(GCNBlock(args.n_layer,\n",
        "                                        args.in_dim if i==0 else args.hidden_dim,\n",
        "                                        args.hidden_dim,\n",
        "                                        args.hidden_dim,\n",
        "                                        args.n_atom,\n",
        "                                        args.bn,\n",
        "                                        args.sc))\n",
        "        self.readout = ReadOut(args.hidden_dim, \n",
        "                               args.pred_dim1,\n",
        "                               act=nn.ReLU())\n",
        "        self.pred1 = Predictor(args.pred_dim1,\n",
        "                               args.pred_dim2,\n",
        "                               act=nn.ReLU())\n",
        "        self.pred2 = Predictor(args.pred_dim2,\n",
        "                               args.pred_dim3,\n",
        "                               act=nn.Tanh())\n",
        "        self.pred3 = Predictor(args.pred_dim3,\n",
        "                               args.out_dim)\n",
        "        \n",
        "    def forward(self, x, adj):\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            out, adj = block((x if i==0 else out), adj)\n",
        "        out = self.readout(out)\n",
        "        out = self.pred1(out)\n",
        "        out = self.pred2(out)\n",
        "        out = self.pred3(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "ph20BK3r1KpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, partition, optimizer, criterion, args):\n",
        "    trainloader = torch.utils.data.DataLoader(partition['train'], \n",
        "                                              batch_size=args.train_batch_size, \n",
        "                                              shuffle=True, num_workers=2)\n",
        "    net.train()\n",
        "\n",
        "    train_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        optimizer.zero_grad() # [21.01.05 오류 수정] 매 Epoch 마다 .zero_grad()가 실행되는 것을 매 iteration 마다 실행되도록 수정했습니다. \n",
        "\n",
        "        # get the inputs\n",
        "        list_feature, list_adj, list_logP = data\n",
        "        list_feature = list_feature.cuda().float()\n",
        "        list_adj = list_adj.cuda().float()\n",
        "        list_logP = list_logP.cuda().float().view(-1, 1)\n",
        "        outputs = net(list_feature, list_adj)\n",
        "\n",
        "        loss = criterion(outputs, list_logP)\n",
        "        train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    train_loss = train_loss / len(trainloader)\n",
        "    return net, train_loss"
      ],
      "metadata": {
        "id": "z2UC1rhK1BFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(net, partition, criterion, args):\n",
        "    valloader = torch.utils.data.DataLoader(partition['val'], \n",
        "                                            batch_size=args.test_batch_size, \n",
        "                                            shuffle=False, num_workers=2)\n",
        "    net.eval()\n",
        "    val_loss = 0 \n",
        "    with torch.no_grad():\n",
        "        for data in valloader:\n",
        "            list_feature, list_adj, list_logP = data\n",
        "            list_feature = list_feature.cuda().float()\n",
        "            list_adj = list_adj.cuda().float()\n",
        "            list_logP = list_logP.cuda().float().view(-1, 1)\n",
        "            \n",
        "            outputs = net(list_feature, list_adj)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "        val_loss = val_loss / len(valloader)\n",
        "    return val_loss"
      ],
      "metadata": {
        "id": "tFyrsiSs1UYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(net, partition, args):\n",
        "    testloader = torch.utils.data.DataLoader(partition['test'], \n",
        "                                             batch_size=args.test_batch_size, \n",
        "                                             shuffle=False, num_workers=2)\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        logP_total = list()\n",
        "        pred_logP_total = list()\n",
        "        for data in testloader:\n",
        "            list_feature, list_adj, list_logP = data\n",
        "            list_feature = list_feature.cuda().float()\n",
        "            list_adj = list_adj.cuda().float()\n",
        "            list_logP = list_logP.cuda().float()\n",
        "            logP_total += list_logP.tolist()\n",
        "            list_logP = list_logP.view(-1, 1)\n",
        "            \n",
        "            outputs = net(list_feature, list_adj)\n",
        "            pred_logP_total += outputs.view(-1).tolist()\n",
        "\n",
        "        mae = mean_absolute_error(logP_total, pred_logP_total)\n",
        "        std = np.std(np.array(logP_total)-np.array(pred_logP_total))\n",
        "    \n",
        "    return mae, std, logP_total, pred_logP_total"
      ],
      "metadata": {
        "id": "bfWmA40i1UVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def experiment(partition, args):\n",
        "  \n",
        "    net = GCNNet()\n",
        "    net.cuda()\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    if args.optim == 'SGD':\n",
        "        optimizer = optim.SGD(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    elif args.optim == 'RMSprop':\n",
        "        optimizer = optim.RMSprop(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    elif args.optim == 'Adam':\n",
        "        optimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    else:\n",
        "        raise ValueError('In-valid optimizer choice')\n",
        "    \n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "        \n",
        "    for epoch in range(args.epoch):  # loop over the dataset multiple times\n",
        "        ts = time.time()\n",
        "        net, train_loss = train(net, partition, optimizer, criterion, args)\n",
        "        val_loss = validate(net, partition, criterion, args)\n",
        "        te = time.time()\n",
        "        \n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        \n",
        "        print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.2f}/{:2.2f}. Took {:2.2f} sec'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\n",
        "        \n",
        "    mae, std, logP_total, pred_logP_total = test(net, partition, args)    \n",
        "    \n",
        "    result = {}\n",
        "    result['train_losses'] = train_losses\n",
        "    result['val_losses'] = val_losses\n",
        "    result['mae'] = mae\n",
        "    result['std'] = std\n",
        "    result['logP_total'] = logP_total\n",
        "    result['pred_logP_total'] = pred_logP_total\n",
        "    return vars(args), result"
      ],
      "metadata": {
        "id": "1x-hXWNJ1USR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pKCaSZnC1UPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8DOJ_JcV1UJi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}