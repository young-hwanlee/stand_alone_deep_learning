{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2_Improve_Test_Accuracy_in_15Lab.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOLoY6JHQ39z7VR+X3r+sxz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/young-hwanlee/stand_alone_deep_learning/blob/main/Assignment2_Improve_Test_Accuracy_in_15Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PD4cIKKvKFCC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import argparse\n",
        "import numpy as np\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29UainWPco7Y"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Cu753dPPKGkV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56967cfe-9600-41ac-c28a-b13109fda565"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),                                                    # RGB : [0~255] -> [0~1]\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])    # RGB : [0~1] -> [-0.5~0.5], and then divide by std = 0.5\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainset, valset = torch.utils.data.random_split(trainset, [40000, 10000])\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "partition = {'train': trainset, 'val':valset, 'test':testset}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxnfFJwBcsAv"
      },
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_G6bZbbkMWWt"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, hid_dim, n_layer, act, dropout, use_bn, use_xavier):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layer = n_layer\n",
        "        self.act = act\n",
        "        self.dropout = dropout\n",
        "        self.use_bn = use_bn\n",
        "        self.use_xavier = use_xavier\n",
        "        \n",
        "        # ====== Create Linear Layers ====== #\n",
        "        self.fc1 = nn.Linear(self.in_dim, self.hid_dim)\n",
        "        \n",
        "        self.linears = nn.ModuleList()\n",
        "        self.bns = nn.ModuleList()\n",
        "        for i in range(self.n_layer-1):\n",
        "            self.linears.append(nn.Linear(self.hid_dim, self.hid_dim))\n",
        "            if self.use_bn:\n",
        "                self.bns.append(nn.BatchNorm1d(self.hid_dim))\n",
        "                \n",
        "        self.fc2 = nn.Linear(self.hid_dim, self.out_dim)\n",
        "        \n",
        "        # ====== Create Activation Function ====== #\n",
        "        if self.act == 'relu':\n",
        "            self.act = nn.ReLU()\n",
        "        elif self.act == 'tanh':\n",
        "            self.act == nn.Tanh()\n",
        "        elif self.act == 'sigmoid':\n",
        "            self.act = nn.Sigmoid()\n",
        "        else:\n",
        "            raise ValueError('No valid activation function selected!')\n",
        "        \n",
        "        # ====== Create Regularization Layer ======= #\n",
        "        self.dropout = nn.Dropout(self.dropout)\n",
        "        if self.use_xavier:\n",
        "            self.xavier_init()\n",
        "          \n",
        "    def forward(self, x):\n",
        "        x = self.act(self.fc1(x))\n",
        "        for i in range(len(self.linears)):\n",
        "            x = self.act(self.linears[i](x))\n",
        "            x = self.bns[i](x)\n",
        "            x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    def xavier_init(self):\n",
        "        for linear in self.linears:\n",
        "            nn.init.xavier_normal_(linear.weight)\n",
        "            linear.bias.data.fill_(0.01)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# construct Testing Model\n",
        "net = MLP(in_dim=3072, out_dim=10, hid_dim=100, n_layer=4, act='relu', dropout=0.1,\n",
        "          use_bn=True, use_xavier=True)\n",
        "\n",
        "print(net)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWQevDKMuDu1",
        "outputId": "bd3c01f4-7886-44c5-f773-5a87fff3f983"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP(\n",
            "  (fc1): Linear(in_features=3072, out_features=100, bias=True)\n",
            "  (linears): ModuleList(\n",
            "    (0): Linear(in_features=100, out_features=100, bias=True)\n",
            "    (1): Linear(in_features=100, out_features=100, bias=True)\n",
            "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
            "  )\n",
            "  (bns): ModuleList(\n",
            "    (0): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
            "  (act): ReLU()\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itGsp6jDWs_a"
      },
      "source": [
        "## Train, Validate, Test and Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "arU-G_uLXUX0"
      },
      "outputs": [],
      "source": [
        "def train(net, partition, optimizer, criterion, args):\n",
        "    trainloader = torch.utils.data.DataLoader(partition['train'], \n",
        "                                              batch_size=args.train_batch_size, \n",
        "                                              shuffle=True,\n",
        "                                              num_workers=2)\n",
        "    net.train()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    train_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        optimizer.zero_grad() # [21.01.05 오류 수정] 매 Epoch 마다 .zero_grad()가 실행되는 것을 매 iteration 마다 실행되도록 수정했습니다. \n",
        "\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.view(-1, 3072)\n",
        "        inputs = inputs.cuda()\n",
        "        labels = labels.cuda()\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = train_loss / len(trainloader)\n",
        "    train_acc = 100 * correct / total\n",
        "    \n",
        "    return net, train_loss, train_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-tZTjsPYXUX0"
      },
      "outputs": [],
      "source": [
        "def validate(net, partition, criterion, args):\n",
        "    valloader = torch.utils.data.DataLoader(partition['val'], \n",
        "                                            batch_size=args.test_batch_size, \n",
        "                                            shuffle=False,\n",
        "                                            num_workers=2)\n",
        "    net.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_loss = 0 \n",
        "    with torch.no_grad():\n",
        "        for data in valloader:\n",
        "            images, labels = data\n",
        "            images = images.view(-1, 3072)\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "            outputs = net(images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss / len(valloader)\n",
        "        val_acc = 100 * correct / total\n",
        "        \n",
        "    return val_loss, val_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "K4WaGlpWXUX1"
      },
      "outputs": [],
      "source": [
        "def test(net, partition, args):\n",
        "    testloader = torch.utils.data.DataLoader(partition['test'], \n",
        "                                             batch_size=args.test_batch_size, \n",
        "                                             shuffle=False,\n",
        "                                             num_workers=2)\n",
        "    net.eval()\n",
        "    \n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images = images.view(-1, 3072)\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "            outputs = net(images)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        test_acc = 100 * correct / total\n",
        "        \n",
        "    return test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LiOCP6TqWw2V"
      },
      "outputs": [],
      "source": [
        "def experiment(partition, args):\n",
        "  \n",
        "    net = MLP(args.in_dim, args.out_dim, args.hid_dim, args.n_layer, args.act, args.dropout, args.use_bn, args.use_xavier)\n",
        "    net.cuda()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if args.optim == 'SGD':\n",
        "        optimizer = optim.SGD(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    elif args.optim == 'RMSprop':\n",
        "        optimizer = optim.RMSprop(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    elif args.optim == 'Adam':\n",
        "        optimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    else:\n",
        "        raise ValueError('In-valid optimizer choice')\n",
        "    \n",
        "    # loop over the dataset multiple times\n",
        "    for epoch in range(args.epoch):\n",
        "        ts = time.time()\n",
        "        net, train_loss, train_acc = train(net, partition, optimizer, criterion, args)\n",
        "        val_loss, val_acc = validate(net, partition, criterion, args)\n",
        "        te = time.time()\n",
        "        print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.2f}/{:2.2f}. Took {:2.2f} sec'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\n",
        "        \n",
        "    test_acc = test(net, partition, args)    \n",
        "    \n",
        "    return train_loss, val_loss, train_acc, val_acc, test_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omgExzmQgU1J"
      },
      "source": [
        "## Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DRoOy_B3Wu7B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "855eef22-0f4b-4fa8-b416-a58c9e07ab72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Namespace(act='relu', dropout=0.2, epoch=10, hid_dim=300, in_dim=3072, l2=1e-05, lr=0.0015, n_layer=3, optim='RMSprop', out_dim=10, test_batch_size=1024, train_batch_size=256, use_bn=True, use_xavier=True)\n",
            "Epoch 0, Acc(train/val): 36.48/39.75, Loss(train/val) 1.78/1.69. Took 13.79 sec\n",
            "Epoch 1, Acc(train/val): 45.42/43.53, Loss(train/val) 1.52/1.56. Took 9.72 sec\n",
            "Epoch 2, Acc(train/val): 49.27/45.37, Loss(train/val) 1.41/1.56. Took 9.73 sec\n",
            "Epoch 3, Acc(train/val): 52.30/47.27, Loss(train/val) 1.34/1.47. Took 9.94 sec\n",
            "Epoch 4, Acc(train/val): 54.85/47.62, Loss(train/val) 1.27/1.53. Took 9.92 sec\n",
            "Epoch 5, Acc(train/val): 57.01/48.12, Loss(train/val) 1.21/1.47. Took 9.61 sec\n",
            "Epoch 6, Acc(train/val): 58.80/48.93, Loss(train/val) 1.16/1.49. Took 9.89 sec\n",
            "Epoch 7, Acc(train/val): 60.83/48.78, Loss(train/val) 1.10/1.48. Took 9.80 sec\n",
            "Epoch 8, Acc(train/val): 62.34/50.17, Loss(train/val) 1.05/1.48. Took 9.78 sec\n",
            "Epoch 9, Acc(train/val): 64.49/47.68, Loss(train/val) 1.00/1.61. Took 9.81 sec\n",
            "\n",
            "Namespace(act='relu', dropout=0.2, epoch=10, hid_dim=500, in_dim=3072, l2=1e-05, lr=0.0015, n_layer=3, optim='RMSprop', out_dim=10, test_batch_size=1024, train_batch_size=256, use_bn=True, use_xavier=True)\n",
            "Epoch 0, Acc(train/val): 35.49/34.18, Loss(train/val) 1.84/1.90. Took 9.78 sec\n",
            "Epoch 1, Acc(train/val): 45.65/41.55, Loss(train/val) 1.52/1.69. Took 9.66 sec\n",
            "Epoch 2, Acc(train/val): 49.31/42.66, Loss(train/val) 1.41/1.65. Took 9.72 sec\n",
            "Epoch 3, Acc(train/val): 52.37/44.56, Loss(train/val) 1.33/1.58. Took 9.70 sec\n",
            "Epoch 4, Acc(train/val): 55.26/47.74, Loss(train/val) 1.25/1.49. Took 10.54 sec\n",
            "Epoch 5, Acc(train/val): 57.44/46.20, Loss(train/val) 1.19/1.57. Took 9.71 sec\n",
            "Epoch 6, Acc(train/val): 59.92/47.55, Loss(train/val) 1.13/1.56. Took 9.81 sec\n",
            "Epoch 7, Acc(train/val): 61.55/50.28, Loss(train/val) 1.07/1.46. Took 9.74 sec\n",
            "Epoch 8, Acc(train/val): 63.37/49.66, Loss(train/val) 1.02/1.50. Took 9.66 sec\n",
            "Epoch 9, Acc(train/val): 65.56/46.58, Loss(train/val) 0.96/1.72. Took 9.71 sec\n",
            "\n",
            "Namespace(act='relu', dropout=0.2, epoch=10, hid_dim=700, in_dim=3072, l2=1e-05, lr=0.0015, n_layer=3, optim='RMSprop', out_dim=10, test_batch_size=1024, train_batch_size=256, use_bn=True, use_xavier=True)\n",
            "Epoch 0, Acc(train/val): 34.44/33.67, Loss(train/val) 1.92/1.92. Took 9.67 sec\n",
            "Epoch 1, Acc(train/val): 44.88/41.29, Loss(train/val) 1.55/1.65. Took 9.78 sec\n",
            "Epoch 2, Acc(train/val): 48.99/44.01, Loss(train/val) 1.42/1.61. Took 9.73 sec\n",
            "Epoch 3, Acc(train/val): 52.68/45.44, Loss(train/val) 1.33/1.52. Took 9.81 sec\n",
            "Epoch 4, Acc(train/val): 55.28/42.12, Loss(train/val) 1.25/1.70. Took 10.87 sec\n",
            "Epoch 5, Acc(train/val): 57.22/47.16, Loss(train/val) 1.20/1.55. Took 9.92 sec\n",
            "Epoch 6, Acc(train/val): 59.80/46.25, Loss(train/val) 1.12/1.58. Took 9.79 sec\n",
            "Epoch 7, Acc(train/val): 62.12/45.52, Loss(train/val) 1.06/1.77. Took 9.78 sec\n",
            "Epoch 8, Acc(train/val): 64.24/44.80, Loss(train/val) 1.00/1.73. Took 9.79 sec\n",
            "Epoch 9, Acc(train/val): 66.57/47.82, Loss(train/val) 0.93/1.66. Took 9.72 sec\n",
            "\n",
            "Namespace(act='relu', dropout=0.2, epoch=10, hid_dim=300, in_dim=3072, l2=1e-05, lr=0.0015, n_layer=4, optim='RMSprop', out_dim=10, test_batch_size=1024, train_batch_size=256, use_bn=True, use_xavier=True)\n",
            "Epoch 0, Acc(train/val): 35.78/38.10, Loss(train/val) 1.79/1.70. Took 9.79 sec\n",
            "Epoch 1, Acc(train/val): 44.98/37.56, Loss(train/val) 1.53/1.78. Took 9.93 sec\n",
            "Epoch 2, Acc(train/val): 49.27/45.25, Loss(train/val) 1.42/1.58. Took 9.86 sec\n",
            "Epoch 3, Acc(train/val): 51.98/43.59, Loss(train/val) 1.34/1.64. Took 9.84 sec\n",
            "Epoch 4, Acc(train/val): 54.33/45.99, Loss(train/val) 1.28/1.53. Took 10.70 sec\n",
            "Epoch 5, Acc(train/val): 56.23/45.03, Loss(train/val) 1.23/1.60. Took 9.91 sec\n",
            "Epoch 6, Acc(train/val): 57.07/48.10, Loss(train/val) 1.20/1.49. Took 9.85 sec\n",
            "Epoch 7, Acc(train/val): 59.97/50.10, Loss(train/val) 1.13/1.41. Took 9.91 sec\n",
            "Epoch 8, Acc(train/val): 61.13/47.82, Loss(train/val) 1.09/1.54. Took 9.83 sec\n",
            "Epoch 9, Acc(train/val): 63.06/45.74, Loss(train/val) 1.04/1.65. Took 9.91 sec\n",
            "\n",
            "Namespace(act='relu', dropout=0.2, epoch=10, hid_dim=500, in_dim=3072, l2=1e-05, lr=0.0015, n_layer=4, optim='RMSprop', out_dim=10, test_batch_size=1024, train_batch_size=256, use_bn=True, use_xavier=True)\n",
            "Epoch 0, Acc(train/val): 34.27/34.57, Loss(train/val) 1.86/1.90. Took 9.82 sec\n",
            "Epoch 1, Acc(train/val): 44.95/35.41, Loss(train/val) 1.54/1.94. Took 9.79 sec\n",
            "Epoch 2, Acc(train/val): 48.93/40.78, Loss(train/val) 1.43/1.69. Took 9.86 sec\n",
            "Epoch 3, Acc(train/val): 52.34/46.03, Loss(train/val) 1.34/1.52. Took 9.83 sec\n",
            "Epoch 4, Acc(train/val): 54.33/48.30, Loss(train/val) 1.27/1.48. Took 10.68 sec\n",
            "Epoch 5, Acc(train/val): 56.82/47.88, Loss(train/val) 1.21/1.52. Took 9.90 sec\n",
            "Epoch 6, Acc(train/val): 58.95/48.80, Loss(train/val) 1.15/1.51. Took 9.87 sec\n",
            "Epoch 7, Acc(train/val): 60.86/49.39, Loss(train/val) 1.09/1.53. Took 9.99 sec\n",
            "Epoch 8, Acc(train/val): 62.97/48.28, Loss(train/val) 1.03/1.53. Took 9.94 sec\n",
            "Epoch 9, Acc(train/val): 64.25/48.38, Loss(train/val) 1.00/1.55. Took 9.90 sec\n",
            "\n",
            "Namespace(act='relu', dropout=0.2, epoch=10, hid_dim=700, in_dim=3072, l2=1e-05, lr=0.0015, n_layer=4, optim='RMSprop', out_dim=10, test_batch_size=1024, train_batch_size=256, use_bn=True, use_xavier=True)\n",
            "Epoch 0, Acc(train/val): 33.65/36.57, Loss(train/val) 1.93/1.80. Took 9.80 sec\n",
            "Epoch 1, Acc(train/val): 43.80/37.41, Loss(train/val) 1.57/1.82. Took 9.80 sec\n",
            "Epoch 2, Acc(train/val): 48.05/40.41, Loss(train/val) 1.45/1.73. Took 9.82 sec\n",
            "Epoch 3, Acc(train/val): 51.11/46.06, Loss(train/val) 1.36/1.54. Took 9.81 sec\n",
            "Epoch 4, Acc(train/val): 53.84/44.79, Loss(train/val) 1.29/1.57. Took 10.81 sec\n",
            "Epoch 5, Acc(train/val): 56.40/41.66, Loss(train/val) 1.23/1.79. Took 9.96 sec\n",
            "Epoch 6, Acc(train/val): 58.84/47.40, Loss(train/val) 1.16/1.57. Took 9.72 sec\n",
            "Epoch 7, Acc(train/val): 60.83/46.65, Loss(train/val) 1.10/1.61. Took 9.87 sec\n",
            "Epoch 8, Acc(train/val): 62.59/48.29, Loss(train/val) 1.05/1.60. Took 9.81 sec\n",
            "Epoch 9, Acc(train/val): 64.75/49.36, Loss(train/val) 0.98/1.56. Took 9.83 sec\n",
            "\n",
            "Namespace(act='relu', dropout=0.2, epoch=10, hid_dim=300, in_dim=3072, l2=1e-05, lr=0.0015, n_layer=5, optim='RMSprop', out_dim=10, test_batch_size=1024, train_batch_size=256, use_bn=True, use_xavier=True)\n",
            "Epoch 0, Acc(train/val): 34.76/31.45, Loss(train/val) 1.82/1.97. Took 9.94 sec\n",
            "Epoch 1, Acc(train/val): 43.51/35.69, Loss(train/val) 1.57/1.89. Took 9.97 sec\n",
            "Epoch 2, Acc(train/val): 48.19/42.41, Loss(train/val) 1.46/1.65. Took 9.85 sec\n",
            "Epoch 3, Acc(train/val): 51.07/45.24, Loss(train/val) 1.38/1.55. Took 9.92 sec\n",
            "Epoch 4, Acc(train/val): 53.45/43.58, Loss(train/val) 1.31/1.82. Took 10.82 sec\n",
            "Epoch 5, Acc(train/val): 55.32/45.55, Loss(train/val) 1.25/1.56. Took 10.03 sec\n",
            "Epoch 6, Acc(train/val): 57.16/46.27, Loss(train/val) 1.21/1.61. Took 9.92 sec\n",
            "Epoch 7, Acc(train/val): 58.84/48.14, Loss(train/val) 1.16/1.47. Took 10.00 sec\n",
            "Epoch 8, Acc(train/val): 61.01/49.62, Loss(train/val) 1.10/1.48. Took 9.92 sec\n",
            "Epoch 9, Acc(train/val): 62.15/50.11, Loss(train/val) 1.07/1.45. Took 9.83 sec\n",
            "\n",
            "Namespace(act='relu', dropout=0.2, epoch=10, hid_dim=500, in_dim=3072, l2=1e-05, lr=0.0015, n_layer=5, optim='RMSprop', out_dim=10, test_batch_size=1024, train_batch_size=256, use_bn=True, use_xavier=True)\n",
            "Epoch 0, Acc(train/val): 33.17/35.11, Loss(train/val) 1.89/1.80. Took 9.87 sec\n",
            "Epoch 1, Acc(train/val): 42.67/42.29, Loss(train/val) 1.59/1.63. Took 10.02 sec\n",
            "Epoch 2, Acc(train/val): 47.13/40.14, Loss(train/val) 1.47/1.66. Took 9.97 sec\n",
            "Epoch 3, Acc(train/val): 50.53/43.04, Loss(train/val) 1.38/1.72. Took 9.85 sec\n",
            "Epoch 4, Acc(train/val): 53.12/45.33, Loss(train/val) 1.31/1.53. Took 10.75 sec\n",
            "Epoch 5, Acc(train/val): 55.43/45.15, Loss(train/val) 1.25/1.61. Took 9.95 sec\n",
            "Epoch 6, Acc(train/val): 57.45/41.81, Loss(train/val) 1.20/1.76. Took 9.85 sec\n",
            "Epoch 7, Acc(train/val): 59.63/45.92, Loss(train/val) 1.14/1.61. Took 9.98 sec\n",
            "Epoch 8, Acc(train/val): 61.36/47.32, Loss(train/val) 1.09/1.63. Took 10.05 sec\n",
            "Epoch 9, Acc(train/val): 63.49/43.14, Loss(train/val) 1.03/1.84. Took 10.02 sec\n",
            "\n",
            "Namespace(act='relu', dropout=0.2, epoch=10, hid_dim=700, in_dim=3072, l2=1e-05, lr=0.0015, n_layer=5, optim='RMSprop', out_dim=10, test_batch_size=1024, train_batch_size=256, use_bn=True, use_xavier=True)\n",
            "Epoch 0, Acc(train/val): 31.69/27.44, Loss(train/val) 1.98/2.30. Took 9.90 sec\n",
            "Epoch 1, Acc(train/val): 42.06/35.77, Loss(train/val) 1.62/1.93. Took 9.88 sec\n",
            "Epoch 2, Acc(train/val): 46.45/36.94, Loss(train/val) 1.51/3.43. Took 9.93 sec\n",
            "Epoch 3, Acc(train/val): 49.71/43.03, Loss(train/val) 1.42/1.66. Took 9.86 sec\n",
            "Epoch 4, Acc(train/val): 52.10/39.72, Loss(train/val) 1.36/2.05. Took 10.85 sec\n",
            "Epoch 5, Acc(train/val): 54.34/41.21, Loss(train/val) 1.29/1.74. Took 9.98 sec\n",
            "Epoch 6, Acc(train/val): 56.45/46.39, Loss(train/val) 1.23/1.57. Took 9.97 sec\n",
            "Epoch 7, Acc(train/val): 58.43/48.37, Loss(train/val) 1.17/1.49. Took 10.01 sec\n",
            "Epoch 8, Acc(train/val): 60.42/45.09, Loss(train/val) 1.12/1.77. Took 9.78 sec\n",
            "Epoch 9, Acc(train/val): 62.80/44.83, Loss(train/val) 1.06/1.89. Took 9.85 sec\n"
          ]
        }
      ],
      "source": [
        "# ====== Random Seed Initialization ====== #\n",
        "seed = 123\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_args(\"\")\n",
        "\n",
        "# ====== Model Capacity ====== #\n",
        "args.in_dim = 3072\n",
        "args.out_dim = 10\n",
        "args.hid_dim = 100\n",
        "args.act = 'relu'\n",
        "\n",
        "# ====== Regularization ======= #\n",
        "args.dropout = 0.2\n",
        "args.use_bn = True\n",
        "args.l2 = 0.00001\n",
        "args.use_xavier = True\n",
        "\n",
        "# ====== Optimizer & Training ====== #\n",
        "args.optim = 'RMSprop' #SGD, RMSprop, ADAM...\n",
        "args.lr = 0.0015\n",
        "args.epoch = 10\n",
        "\n",
        "args.train_batch_size = 256\n",
        "args.test_batch_size = 1024\n",
        "\n",
        "# ====== Experiment Variable ====== #\n",
        "name_var1 = 'n_layer'\n",
        "name_var2 = 'hid_dim'\n",
        "list_var1 = [3, 4, 5]\n",
        "list_var2 = [300, 500, 700]\n",
        "\n",
        "\n",
        "for var1 in list_var1:\n",
        "    for var2 in list_var2:\n",
        "        setattr(args, name_var1, var1)\n",
        "        setattr(args, name_var2, var2)\n",
        "\n",
        "        print()\n",
        "        print(args)\n",
        "        result = experiment(partition, args)  "
      ]
    }
  ]
}